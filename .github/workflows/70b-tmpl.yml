name: Template - LLaMA 70B

on:
  workflow_call:
    inputs:
      exp-name:
        required: true
        type: string
      isl:
        required: true
        type: string
      osl:
        required: true
        type: string
      max-model-len:
        required: true
        type: string
      random-range-ratio:
        required: true
        type: string

      use_h100:
        type: boolean
        required: true
      use_h200:
        type: boolean
        required: true
      use_b200:
        type: boolean
        required: true
      use_mi300x:
        type: boolean
        required: true
      use_mi325x:
        type: boolean
        required: true
      use_mi355x:
        type: boolean
        required: true
      use_gaudi3:
        type: boolean
        required: true

jobs:
  bmk-h100-fp8:
    if: ${{ inputs.use_h100 }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: h100
      image: 'vllm/vllm-openai:v0.10.2'
      model: 'nvidia/Llama-3.3-70B-Instruct-FP8'
      framework: 'vllm'
      precision: 'fp8'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[2, 4, 8]'

  bmk-h200-fp8:
    if: ${{ inputs.use_h200 }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: h200
      image: 'vllm/vllm-openai:v0.10.2'
      model: 'nvidia/Llama-3.3-70B-Instruct-FP8'
      framework: 'vllm'
      precision: 'fp8'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 2, 4, 8]'

  bmk-h200-trt-fp8:
    if: ${{ inputs.use_h200 }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: h200-trt
      image: 'nvcr.io#nvidia/tensorrt-llm/release:1.1.0rc2.post2'
      model: 'nvidia/Llama-3.3-70B-Instruct-FP8'
      framework: 'trt'
      precision: 'fp8'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 2, 4, 8]'  
      conc-list: '[4, 8, 16, 32, 64, 128]'  # H200 can achieve TPS/User >= 30 with larger concurrency till 128

  bmk-b200-fp8:
    if: ${{ inputs.use_b200 }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: b200
      image: 'vllm/vllm-openai:v0.10.2'
      model: 'nvidia/Llama-3.3-70B-Instruct-FP8'
      framework: 'vllm'
      precision: 'fp8'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 2, 4, 8]' # fix: add TP=2,4 to B200, just as mi355 has

  bmk-b200-trt-fp8:
    if: ${{ inputs.use_b200 }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: b200-trt
      image: 'nvcr.io#nvidia/tensorrt-llm/release:1.1.0rc2.post2'
      model: 'nvidia/Llama-3.3-70B-Instruct-FP8'
      framework: 'trt'
      precision: 'fp8'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 2, 4, 8]' # fix: add TP=2,4 to B200, just as mi355 has
      conc-list: '[4, 8, 16, 32, 64, 128]'  # B200 can achieve TPS/User >= 30 with larger concurrency till 256

  bmk-mi300x-fp8:
    if: ${{ inputs.use_mi300x }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: mi300x
      image: 'rocm/7.0:rocm7.0_ubuntu_22.04_vllm_0.10.1_instinct_20250927_rc1'
      model: 'amd/Llama-3.3-70B-Instruct-FP8-KV'
      framework: 'vllm'
      precision: 'fp8'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 2, 4, 8]'

  bmk-mi325x-fp8:
    if: ${{ inputs.use_mi325x }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: mi325x
      image: 'rocm/7.0:rocm7.0_ubuntu_22.04_vllm_0.10.1_instinct_20250927_rc1'
      model: 'amd/Llama-3.3-70B-Instruct-FP8-KV'
      framework: 'vllm'
      precision: 'fp8'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 2, 4, 8]'

  bmk-mi355x-fp8:
    if: ${{ inputs.use_mi355x }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: mi355x
      image: 'rocm/7.0:rocm7.0_ubuntu_22.04_vllm_0.10.1_instinct_20250927_rc1'
      model: 'amd/Llama-3.3-70B-Instruct-FP8-KV'
      framework: 'vllm'
      precision: 'fp8'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 2, 4, 8]'

  # ========== GAUDI3 JOBS ==========
  bmk-gaudi3-fp8:
    if: ${{ inputs.use_gaudi3 }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: self-hosted
      image: 'vault.habana.ai/gaudi-docker/1.22.0/ubuntu24.04/habanalabs/vllm-installer-2.7.1:latest'
      model: 'meta-llama/Llama-3.3-70B-Instruct'
      framework: 'vllm'
      precision: 'fp8'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[2,4,8]'
  # =================================

  bmk-b200-fp4:
    if: ${{ inputs.use_b200 }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: b200
      image: 'vllm/vllm-openai:v0.10.2'
      model: 'nvidia/Llama-3.3-70B-Instruct-FP4'
      framework: 'vllm'
      precision: 'fp4'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 2, 4, 8]'  # fix: add TP=2,4 to B200, just as mi355 has

  bmk-b200-trt-fp4:
    if: ${{ inputs.use_b200 }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: b200-trt
      image: 'nvcr.io#nvidia/tensorrt-llm/release:1.1.0rc2.post2'
      model: 'nvidia/Llama-3.3-70B-Instruct-FP4'
      framework: 'trt'
      precision: 'fp4'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 2, 4, 8]' # fix: add TP=2,4 to B200, just as mi355 has
      conc-list: '[4, 8, 16, 32, 64, 128]'  # B200 can achieve TPS/User >= 30 with larger concurrency till 128

  bmk-mi355x-fp4:
    if: ${{ inputs.use_mi355x }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: mi355x
      image: 'rocm/7.0:rocm7.0_ubuntu_22.04_vllm_0.10.1_instinct_20250927_rc1'
      model: 'amd/Llama-3.3-70B-Instruct-MXFP4-Preview'
      framework: 'vllm'
      precision: 'fp4'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 2, 4, 8]'
