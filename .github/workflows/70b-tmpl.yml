name: Template - LLaMA 70B

on:
  workflow_call:
    inputs:
      exp-name:
        required: true
        type: string
      isl:
        required: true
        type: string
      osl:
        required: true
        type: string
      max-model-len:
        required: true
        type: string
      random-range-ratio:
        required: true
        type: string

      use_h100:
        type: boolean
        required: true
      use_h200:
        type: boolean
        required: true
      use_b200:
        type: boolean
        required: true
      use_mi300x:
        type: boolean
        required: true
      use_mi325x:
        type: boolean
        required: true
      use_mi355x:
        type: boolean
        required: true

jobs:
  bmk-h100-fp8:
    if: ${{ inputs.use_h100 }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: h100
      image: 'kedarpotdar147/vllm0.1:latest'
      model: 'nvidia/Llama-3.3-70B-Instruct-FP8'
      framework: 'vllm'
      precision: 'fp8'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[2, 4, 8]'

  bmk-h200-fp8:
    if: ${{ inputs.use_h200 }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: h200
      image: 'vllm/vllm-openai:v0.10.2'
      model: 'nvidia/Llama-3.3-70B-Instruct-FP8'
      framework: 'vllm'
      precision: 'fp8'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 2, 4, 8]'

  bmk-h200-trt-fp8:
    if: ${{ inputs.use_h200 }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: h200-trt
      image: 'nvcr.io#nvidia/tensorrt-llm/release:1.1.0rc2.post2'
      model: 'nvidia/Llama-3.3-70B-Instruct-FP8'
      framework: 'trt'
      precision: 'fp8'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 2, 4, 8]'  
      conc-list: '[4, 8, 16, 32, 64, 128]'  # H200 can achieve TPS/User >= 30 with larger concurrency till 128

  bmk-b200-fp8:
    if: ${{ inputs.use_b200 }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: b200
      image: 'vllm/vllm-openai:v0.10.2'
      model: 'nvidia/Llama-3.3-70B-Instruct-FP8'
      framework: 'vllm'
      precision: 'fp8'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 8]'

  bmk-b200-trt-fp8:
    if: ${{ inputs.use_b200 }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: b200
      image: 'nvcr.io#nvidia/tensorrt-llm/release:1.1.0rc2.post2'
      model: 'nvidia/Llama-3.3-70B-Instruct-FP8'
      framework: 'trt'
      precision: 'fp8'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 8]'  
      conc-list: '[4, 8, 16, 32, 64, 128, 256]'  # B200 can achieve TPS/User >= 30 with larger concurrency till 256

  bmk-mi300x-fp8:
    if: ${{ inputs.use_mi300x }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: mi300x
      image: 'rocm/7.0:rocm7.0_ubuntu_22.04_vllm_0.10.1_instinct_20250915'
      model: 'amd/Llama-3.3-70B-Instruct-FP8-KV'
      framework: 'vllm'
      precision: 'fp8'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 2, 4, 8]'

  bmk-mi325x-fp8:
    if: ${{ inputs.use_mi325x }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: mi325x
      image: 'rocm/7.0:rocm7.0_ubuntu_22.04_vllm_0.10.1_instinct_20250915'
      model: 'amd/Llama-3.3-70B-Instruct-FP8-KV'
      framework: 'vllm'
      precision: 'fp8'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 2, 4, 8]'

  bmk-mi355x-fp8:
    if: ${{ inputs.use_mi355x }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: mi355x
      image: 'rocm/7.0:rocm7.0_ubuntu_22.04_vllm_0.10.1_instinct_20250915'
      model: 'amd/Llama-3.3-70B-Instruct-FP8-KV'
      framework: 'vllm'
      precision: 'fp8'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 2, 4, 8]'

  bmk-b200-fp4:
    if: ${{ inputs.use_b200 }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: b200
      image: 'vllm/vllm-openai:v0.10.2'
      model: 'nvidia/Llama-3.3-70B-Instruct-FP4'
      framework: 'vllm'
      precision: 'fp4'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 8]'  

  bmk-b200-trt-fp4:
    if: ${{ inputs.use_b200 }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: b200
      image: 'nvcr.io#nvidia/tensorrt-llm/release:1.1.0rc2.post2'
      model: 'nvidia/Llama-3.3-70B-Instruct-FP4'
      framework: 'trt'
      precision: 'fp4'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 8]'  
      conc-list: '[4, 8, 16, 32, 64, 128, 256]'  # B200 can achieve TPS/User >= 30 with larger concurrency till 256

  bmk-mi355x-fp4:
    if: ${{ inputs.use_mi355x }}
    uses: ./.github/workflows/benchmark-tmpl.yml
    secrets: inherit
    with:
      runner: mi355x
      image: 'rocm/7.0:rocm7.0_ubuntu_22.04_vllm_0.10.1_instinct_20250915'
      model: 'amd/Llama-3.3-70B-Instruct-MXFP4-Preview'
      framework: 'vllm'
      precision: 'fp4'
      exp-name: ${{ inputs.exp-name }}
      isl: ${{ inputs.isl }}
      osl: ${{ inputs.osl }}
      max-model-len: ${{ inputs.max-model-len }}
      random-range-ratio: ${{ inputs.random-range-ratio }}
      tp-list: '[1, 2, 4, 8]'
